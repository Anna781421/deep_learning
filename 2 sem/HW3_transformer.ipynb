{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c_2w1kcg0Wv"
      },
      "source": [
        "# Трансформеры\n",
        "В этом домашнем задании мы рассмотим использование трансформеров в библиотеке PyTorch. Рассмотрим задачу языкового моделирования. Попробуем генерировать текст нейронной сетью. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pg7GXQTg0Wy"
      },
      "source": [
        "Ссылка на данные - https://drive.google.com/drive/folders/1x1A4ElliUGBPnHladGMwPxPuGxI8Vnpu?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K4mhY8pg0Wz"
      },
      "outputs": [],
      "source": [
        "# хороший тон, импортировать все необходимые библиотеки в одной ячейке ;)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5CxPTJBFN9z",
        "outputId": "7a4427bf-ea54-41da-eb1f-482053fe93f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-20 16:38:43--  https://docs.google.com/uc?export=download&id=1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.124.139, 74.125.124.138, 74.125.124.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.124.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sgae7dtk2vfqrgfkl4tge9uj9avkv7rj/1640018325000/09709797766287458306/*/1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-12-20 16:38:45--  https://doc-0s-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sgae7dtk2vfqrgfkl4tge9uj9avkv7rj/1640018325000/09709797766287458306/*/1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX?e=download\n",
            "Resolving doc-0s-54-docs.googleusercontent.com (doc-0s-54-docs.googleusercontent.com)... 209.85.200.132, 2607:f8b0:4001:c16::84\n",
            "Connecting to doc-0s-54-docs.googleusercontent.com (doc-0s-54-docs.googleusercontent.com)|209.85.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56028286 (53M) [text/plain]\n",
            "Saving to: ‘data1’\n",
            "\n",
            "data1               100%[===================>]  53.43M   125MB/s    in 0.4s    \n",
            "\n",
            "2021-12-20 16:38:45 (125 MB/s) - ‘data1’ saved [56028286/56028286]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX' -O data1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFGIoKXEg0W1"
      },
      "source": [
        "Что такое языковое моделирование? Это предсказание вероятности следующего токена (слова или буквы) на основе предыдущих токенов. Математически это можно описать так:\n",
        "\n",
        "$$P(x_i|x_1, x_2 , ... , x_{i-1})$$ \n",
        "\n",
        "Последовательность $$ x_1, x_2, ... x_{i-1} $$ называют контекстом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2v7S9GGg0W5"
      },
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Мы будем обучать языковую модель для предсказания следущей буквы. Такие языковые модели применяются в распозновании речи, так как предоставляют дополнительную информацию акустической модели при выборе следующего символа. Для начала, откройте файл с данными, посмотрите, какие символы входят в тексты, сколько их. Уберите из текста все символы переноса на новую строку и табуляцию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxX5s6R6g0W6",
        "outputId": "adbb0edb-1772-4325-8965-3fd1aaaafa0a",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "path = 'data1'\n",
        "file = open(path, 'r')\n",
        "data = file.readlines()\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKqneAyvaL1H",
        "outputId": "3120443f-eead-4d64-acff-d8d2e46399a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'з', 'л', 'с', 'м', 'к', 'а', 'ю', 'ъ', 'ш', 'ё', 'е', 'д', 'т', 'н', 'й', 'у', 'ы', 'ь', 'ф', 'г', 'ж', '-', ' ', 'п', 'р', 'и', 'о', 'б', 'я', 'ц', 'в', 'э', 'ч', 'х', '\\n', 'щ'}\n"
          ]
        }
      ],
      "source": [
        "print(set(''.join(data))) # Смотрим какие символы содержит текст. Из \"шумовых\" символов только перенос строки \"\\n\" и дефис \"-\".\n",
        "# Дефис не относится к табуляции, также как и пробел. Видимо, мы их оставляем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BWUXZ0hHx3f"
      },
      "outputs": [],
      "source": [
        "data = [i.strip('\\n') for i in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKr97UjHbztT",
        "outputId": "1c210dfb-628f-4ebe-b721-5b719fb13004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ],
      "source": [
        "print(len(set(''.join(data)))) # остаются 33 буквы алфавита, пробел, и дефис. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZSXoUQPg0W7"
      },
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Для обучения модели требуется сначала подготовить текст в подходящий для нейросети вид. Важно также отметить, что нужно добавить два токена start и end, которые отвечают за начало и конец текста. Используйте [ и ] для этой задачи. Также нам нужен токен pad, чтобы заполнять им текст до требуемой длинны для формирования батча.\n",
        "\n",
        "Реализуйте метод preprocess класса Preprocessor. Он должен принимать на вход текст и длинну текста, которую мы ожидаем получить на выходе. Текст должен быть переведен в нижний регистр, в конец текста добавляется требуемое число pad токенов, далее текст векторизуется (каждому символу ставится свое число). Вернуть требуется два вектора. Полученный результат без последнего токена (на нем будем обучаться) и полученный результат без первого токена (целевые метки при обучении)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CpTxmHag0W8"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self):\n",
        "        self.alphabet = '_добсркгаупитнезчм яфжлйвцыэь-шхющёъ]['\n",
        "        self.token2ind = {}\n",
        "        self.ind2token = {}\n",
        "        for i in range(len(self.alphabet)):\n",
        "            self.token2ind[self.alphabet[i]] = i\n",
        "            self.ind2token[i] = self.alphabet[i]\n",
        "        \n",
        "    \n",
        "    def preprocess(self, text, window_size):\n",
        "        text = '[' + text + ']' # Добавляю токены начала (\"[\") и конца (\"]\") текста\n",
        "        if len(text) < window_size: \n",
        "            text += '_'*(window_size - len(text)) # Заполняю недостающие символы pad токенами \"_\"\n",
        "        vector = np.array([self.token2ind[tol] for tol in text.lower()]) # Перевожу текст в векторный вид\n",
        "        self.train_vec = vector[:-1]\n",
        "        self.test_vec = vector[1:]\n",
        "        return self.train_vec, self.test_vec\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPILEPA9g0W9"
      },
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Так как мы решили, что текст будет начинаться токеном [ и заканчиваться токеном ], данные нужно поправить. Реализуйте эту идею, добавьте данные токены в ваши тексты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqhNEHaUkasS"
      },
      "source": [
        "Токены начала и конца текста добавляются на этапе препроцессинга. Реализовано в задании 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATVs2Cmkg0W-"
      },
      "source": [
        "## Задание 4 (0.5 балла)\n",
        "Так как мы не располагаем большими мощностями, то давайте ограничим максимальную длинну текста. Вы можете менять этот порог и тем самым уменьшать кол-во текстов в вашей выборке и увеличивая тем самым скорость обучения. Начнем же мы с 128. \n",
        "Выберите порог и оставьте только те тексты, длина которых не превосходит данный порог.\n",
        "\n",
        "Далее разбейте тексты на train и test, перемешайте тексты при разбиении, размер тестовой выборки должен быть 15% от общего числа текстов. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxL41jKwg0W_"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 128\n",
        "\n",
        "data_cut = []\n",
        "\n",
        "for text in data:\n",
        "  if len(text) < THRESHOLD - 2: #Сделала -2, т.к. добавляю токены конца и начала т екста на этапе препроцессинга. \n",
        "    data_cut.append(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6RU-xAkPFI8",
        "outputId": "1d20f8dd-fd01-471e-a60e-37c34bbdefd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "683133"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(data_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTaHUMZprHDe"
      },
      "outputs": [],
      "source": [
        "data_train, data_test = train_test_split(data_cut, test_size=0.15, random_state=np.random.seed(), shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGJTTKTEg0W_"
      },
      "source": [
        "## Задание 5 (2 балла)\n",
        "Напишем датасет. На вход датасету передается набор текстов, объект класса Preprocessor и размер окна, который вы выбрали в прошлом задании.\n",
        "Реализуйте методы __len__ и __getitem__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-L7CM8jg0XA"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, data, preproc, win_size = 128):\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        for text in data:\n",
        "          x, y = preproc.preprocess(text, win_size)\n",
        "          self.x.append(x)\n",
        "          self.y.append(y)\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLyHDEGLg0XB"
      },
      "outputs": [],
      "source": [
        "preproc = Preprocessor()\n",
        "train_dataset = TextDataset(data_train, preproc)\n",
        "test_dataset = TextDataset(data_test, preproc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNJDvElEg0XB"
      },
      "source": [
        "## Задание 6 (2 балла)\n",
        "Напишем модель. Класс для реализации positional encoding реализован за вас, он нужен, чтобы модель могла после получения эмбедингов понимать, на каком месте какой токен находится.\n",
        "\n",
        "Заполните пропуски в классе модели. Гипперпараметры модели вам предлагается подобрать самостоятельно. Рекомендуется использовать не более 6 слоев в трансформере. В декореде испоьлзуйте две линейных слоя с функцией активации ReLU между ними.\n",
        "\n",
        "## Задание 6_1 (0 баллов, но надо ответить!)\n",
        "При обучении языковой модели на основе трансформеров мы используем маскирование символов (как мы это делаем - уже реализовано). Напишите, почему мы это делаем? Почему это так важно?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yGGMN2AOrEr"
      },
      "source": [
        "Маскирование необходимо чтобы ограничить доступ нашей модели к последующим символам последовательности. Мы хотим, чтобы модель обучалась на уже известных символах, не заглядывая в последующий текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tFCd-Szg0XC"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rEIE8cpg0XD"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        d_model=16\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=2, dropout=0.2)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.transformer_encoder_layer, num_layers=2)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model, vocab_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(vocab_size, vocab_size)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.pe(self.emb(x)) # emb, then pe\n",
        "        x = x.transpose(1, 0)\n",
        "        x = self.transformer_encoder(self.transformer_encoder_layer(x), src_mask) # transformer encoder with mask\n",
        "        x = self.decoder(x) # decoder\n",
        "        return x.transpose(1, 0)\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxmD5ftHg0XE"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len('_добсркгаупитнезчм яфжлйвцыэь-шхющёъ]['))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIjt-fjdg0XF"
      },
      "source": [
        "## Задание 7 (2,5 балла)\n",
        "Финишная прямая. Давайте реализуем класс для обучения модели и ее валидации. Следуйте указаниям в коде и заполните недостающие фрагменты в коде."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvBnzisig0XF"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, train_dataset, test_dataset):\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        self.train_batch_size = 127\n",
        "        self.test_batch_size = 127\n",
        "        \n",
        "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=self.train_batch_size, drop_last=True, shuffle=True)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=self.test_batch_size, drop_last=True, shuffle=True)\n",
        "        self.train_dataloader_size = len(train_dataset) // self.train_batch_size\n",
        "        self.test_dataloader_size = len(test_dataset) // self.test_batch_size\n",
        "        \n",
        "        self.device = 'cuda:0'\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        \n",
        "        self.optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1.0, gamma=0.8)\n",
        "        \n",
        "        self.steps_to_print = 1000\n",
        "        \n",
        "    def train_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        scr_mask = self.model.generate_square_subsequent_mask(self.train_batch_size).to(self.device)\n",
        "        \n",
        "        for batch in self.train_dataloader:\n",
        "            x, y = batch\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            predicted = model(x, scr_mask) \n",
        "            loss = self.criterion(predicted.reshape(16129, 38), y.view(127*127).long())\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            counted_loss += loss.item()\n",
        "            step+=1\n",
        "            it+=1\n",
        "            \n",
        "            \n",
        "            if step%self.steps_to_print == 0:\n",
        "                it = step\n",
        "                result = 'Train epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "    \n",
        "    def validate_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        scr_mask = self.model.generate_square_subsequent_mask(self.test_batch_size).to(self.device)\n",
        "        \n",
        "        for batch in self.test_dataloader:\n",
        "            x, y = batch\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            predicted = model(x, scr_mask)\n",
        "            loss = self.criterion(predicted.reshape(16129, 38), y.view(127*127).long())\n",
        "            counted_loss += loss.item()\n",
        "            step+=1\n",
        "            it+=1\n",
        "            \n",
        "            \n",
        "            if step%(self.steps_to_print//2) == 0:\n",
        "                it = step\n",
        "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "        \n",
        "    def train(self, number_of_epochs):\n",
        "        self.model.to(self.device)\n",
        "        for epoch in range(1, number_of_epochs+1):\n",
        "            model.train()\n",
        "            self.train_one_epoch(epoch)\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                self.validate_one_epoch(epoch)\n",
        "            print()\n",
        "            self.scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7hz7uBIg0XH"
      },
      "source": [
        "Что такое ppl? Перплексия. Ее можно интерпретировать как меру \"удивленности\" модели нужному символу. Чем меньше данная величина, тем лучше, ведь это значит, что модель если и сделала неправильный выбор, то не сильно удивлена своей ошибке.\n",
        "\n",
        "Проведите несколько экспериментов, посмотрите, при каких гипперпараметрах значение перплексии минимально."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsfRbkvGg0XI"
      },
      "source": [
        "## Задание 8 (0.5 балла)\n",
        "Запустите обучение на нескольких эпохах. Ориентируйтесь на ваши вычислительные мощности и время работы. Вы всегда можете посчитать, сколько секунд уходит на один батч."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNfuMWyQg0XI",
        "outputId": "ea37c4be-7f3a-4d6a-d9d8-5e4b1fb41adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 1000/4572 | Counted loss 2751.056639432907 | ppl 15.659169241344655 | time 114.18730092048645 | \n",
            "Train epoch 1 | Step 2000/4572 | Counted loss 2611.32036280632 | ppl 3.6901244412624488 | time 113.76406860351562 | \n",
            "Train epoch 1 | Step 3000/4572 | Counted loss 2582.0330657958984 | ppl 2.364762723542021 | time 113.68537139892578 | \n",
            "Train epoch 1 | Step 4000/4572 | Counted loss 2564.8264105319977 | ppl 1.8987705592164652 | time 113.70761322975159 | \n",
            "Validate epoch 1 | Step 500/806 | Counted loss 1256.403660774231 | ppl 12.339522489138021 | time 20.42834782600403 | \n",
            "\n",
            "Train epoch 2 | Step 1000/4572 | Counted loss 2544.4171607494354 | ppl 12.735802996585052 | time 113.83225846290588 | \n",
            "Train epoch 2 | Step 2000/4572 | Counted loss 2537.8405928611755 | ppl 3.5570099719363846 | time 113.6880292892456 | \n",
            "Train epoch 2 | Step 3000/4572 | Counted loss 2530.6955931186676 | ppl 2.324640021771023 | time 113.6796498298645 | \n",
            "Train epoch 2 | Step 4000/4572 | Counted loss 2524.750002384186 | ppl 1.8798415673340234 | time 113.6929202079773 | \n",
            "Validate epoch 2 | Step 500/806 | Counted loss 1239.158779144287 | ppl 11.921190827563189 | time 20.423346757888794 | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "trainer = Trainer(model, train_dataset, test_dataset)\n",
        "trainer.train(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTnKP6h9g0XJ"
      },
      "source": [
        "## Задание 9 (1 балл)\n",
        "Итак, давайте попробуем погенерировать текст нашей сеткой. Закончите функцию по генерации текста. Попробуйте сгенерировать какой-нибудь текст. Помните, что если вы хотите генерировать текст с нуля, то вы должны передать в качестве текста только токен start.\n",
        "Прекратите генерировать текст, если модель выдала токен end или длинна текста больше 150."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Столкнулась с проблемой \"CUDA out of memory\", пришлось ограничить число символов меньшим порогом."
      ],
      "metadata": {
        "id": "C1tL4MBSmKFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FH8h04cg0XK"
      },
      "outputs": [],
      "source": [
        "from torch._C import device\n",
        "def generate_text(text):\n",
        "    x = []\n",
        "    \n",
        "    for letter in text:\n",
        "        x.append(preproc.token2ind[letter])\n",
        "    x = torch.from_numpy(np.array(x))\n",
        "    \n",
        "    pred = model(x.to('cuda:0'), model.generate_square_subsequent_mask(len(text)).to('cuda:0'))\n",
        "    ind = int(torch.argmax(pred[-1][-1].cpu()))\n",
        "\n",
        "    text += preproc.ind2token[ind]\n",
        "    \n",
        "    if text[-1] == ']' or len(text) > 100:\n",
        "        return text\n",
        "    else:\n",
        "        return generate_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF6IJumM9DG-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9dbb5978-55b9-45b0-d281-5770166d27bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[на по по по пото по по по по по по по пото по по по по по по пото по по по по по по по по по по по п'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "generate_text('[')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW3_transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}